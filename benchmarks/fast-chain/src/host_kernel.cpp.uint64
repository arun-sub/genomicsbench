#include <vector>
#include <ctime>
#include <cstdio>
#include <cstdlib>
#include <limits>
#include <cmath>
#include "omp.h"
#include "host_kernel.h"
#include "common.h"
// #include "minimap.h"
// #include "mmpriv.h"
// #include "kalloc.h"

#ifdef __AVX2__
    #include <immintrin.h>
#endif
#ifdef __AVX512BW__
    #include <zmmintrin.h>
#endif
#ifdef __ARM_FEATURE_SVE
    #include <arm_sve.h>
    #define VL svcntd()
#endif

static const char LogTable256_dp_lib[256] = {
#define LT(n) n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n
    -1, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,
        LT(4), LT(5), LT(5), LT(6), LT(6), LT(6), LT(6),
        LT(7), LT(7), LT(7), LT(7), LT(7), LT(7), LT(7), LT(7)
    };

static inline int ilog2_32_dp_lib(uint32_t v) {
    uint32_t t, tt;
    if ((tt = v >> 16)) {
        return (t = tt >> 8) ? 24 + LogTable256_dp_lib[t] : 16 + LogTable256_dp_lib[tt];
    }
    return (t = v >> 8) ? 8 + LogTable256_dp_lib[t] : LogTable256_dp_lib[v];
}

static inline int32_t ilog2_32(uint32_t v) {
    constexpr uint32_t base = 31;
    const uint32_t leading_zeros = __builtin_clz(v);

    return base - leading_zeros;
}

const int BACKSEARCH = 65;
#define MM_SEED_SEG_SHIFT  48
#define MM_SEED_SEG_MASK   (0xffULL<<(MM_SEED_SEG_SHIFT))

#if 0
void print_vector(svint64_t v) {
    int64_t _v[VL];
    svst1_s64(svptrue_b64(),_v,v);
    for (uint64_t i = 0; i < VL; i++) {
        printf("%ld,",_v[i]);
    }
    printf("\n");
}

void print_vector_f(svfloat64_t v) {
    float64_t _v[VL];
    svst1_f64(svptrue_b64(),_v,v);
    for (uint64_t i = 0; i < VL; i++) {
        printf("%f,",_v[i]);
    }
    printf("\n");
}
#endif

static void chain_dp(call_t *a, return_t *ret) {
    // constexpr float gap_scale = 1.0f;
    // constexpr int max_skip = 25;
    // constexpr int is_cdna = 0;
    constexpr int max_iter = 5000;

    const auto max_dist_x = a->max_dist_x;
    const auto max_dist_y = a->max_dist_y;
    const auto bw = a->bw;

    const auto avg_qspan = a->avg_qspan;
    const auto avg_qspan001 = 0.01f * avg_qspan;

    // const auto n_segs = a->n_segs;
    const auto n = a->n;

    auto *anchors_x = a->anchors_x.data();
    auto *anchors_y = a->anchors_y.data();
    auto *anchors_l = a->anchors_l.data();

    ret->n = n;
    ret->scores.resize(n);
    ret->parents.resize(n);
    ret->targets.resize(n);
    ret->peak_scores.resize(n);

    int64_t st = 0;

#if __ARM_FEATURE_SVE
    printf("Executing SVE version");

    // fill the score and backtrack arrays
    for (int64_t i = 0; i < n; ++i) {
        const auto ri_scalar = anchors_x[i];
        svint64_t ri = svdup_n_s64(ri_scalar);
        const int32_t qi_scalar = static_cast<int32_t>(anchors_y[i]);
        svint64_t qi = svdup_n_s64(qi_scalar);
        const int32_t q_spani = qspans[i];

        int64_t max_j = -1;
        int32_t max_f = q_spani;

        while (st < i && ri_scalar > anchors_x[st] + max_dist_x) {
            ++st; //predecessor's position is too far
        }
        if (i - st > max_iter) {
            st = i - max_iter; //predecessor's index is too far
        }

        //for (int64_t j = i - 1; j >= st; --j) {
        svbool_t ptrue = svptrue_b64();
        for (int64_t j = i - 1; j >= st; j-=VL) {
            int64_t real_j = j-VL+1;
            svbool_t valid_elements = svnot_b_z(ptrue,svwhilelt_b64_s64(real_j,st));
            //const auto rj = anchors_x[j];
            svint64_t rj = svld1_s64(valid_elements,(int64_t*)&anchors_x[real_j]);
            //const int32_t qj = static_cast<int32_t>(anchors_y[j]);
            svint64_t qj = svld1_s64(valid_elements,(int64_t*)&anchors_y[real_j]);
            qj = svextw_s64_x(valid_elements,qj);

            //const int64_t dr = ri - rj;
            svint64_t dr = svsub_s64_x(valid_elements,ri,rj);
            //const int32_t dq = qi - qj;
            svint64_t dq = svsub_s64_x(valid_elements,qi,qj);

            //const int32_t dd = std::abs(dr - dq);
            svint64_t dd = svabd_s64_x(valid_elements,dr,dq);

            //if ((dr == 0 || dq <= 0) ||
            //   (dq > max_dist_y || dq > max_dist_x) ||
            //   (dd > bw)) {
            //    continue;
            //}
            svbool_t skip_anchor = svcmpeq_n_s64(valid_elements,dr,0);
            valid_elements = svbic_b_z(valid_elements,valid_elements,skip_anchor);

            skip_anchor = svcmple_n_s64(valid_elements,dq,0);
            valid_elements = svbic_b_z(valid_elements,valid_elements,skip_anchor);

            skip_anchor = svcmpgt_n_s64(valid_elements,dq,max_dist_y);
            valid_elements = svbic_b_z(valid_elements,valid_elements,skip_anchor);

            skip_anchor = svcmpgt_n_s64(valid_elements,dq,max_dist_x);
            valid_elements = svbic_b_z(valid_elements,valid_elements,skip_anchor);

            skip_anchor = svcmpgt_n_s64(valid_elements,dd,bw);
            valid_elements = svbic_b_z(valid_elements,valid_elements,skip_anchor);

            if (!svptest_any(ptrue,valid_elements)) continue;

            //const int64_t dr_dq_min = (dr < dq) ? dr : dq;
            svint64_t dr_dq_min = svmin_s64_x(valid_elements,dr,dq);
            //const int32_t oc = (dr_dq_min < q_spani) ? dr_dq_min : q_spani;
            svint64_t oc = svmin_n_s64_x(valid_elements,dr_dq_min,q_spani);

            //const int32_t log_dd = (dd) ? ilog2_32(dd) : 0;
            svbool_t valid_log = svcmpne_n_s64(valid_elements,dd,0);
            svint64_t log_dd = svreinterpret_s64(svclz_s64_x(valid_elements,dd));
            log_dd = svsubr_n_s64_z(valid_log,log_dd,63);
            
            //const int32_t gap_cost = static_cast<int>(dd * 0.01f * avg_qspan) + (log_dd >> 1);
            /*
            svint64_t gap_cost = svmul_n_s64_x(valid_elements,dd,avg_qspan001);
            log_dd = svlsr_n_s64_x(valid_elements,log_dd,1);
            gap_cost = svadd_s64_x(valid_elements,gap_cost,log_dd);
            */
            svfloat64_t gap_cost_f = svcvt_f64_s64_x(valid_elements,dd);
            //gap_cost_f = svmul_n_f64_x(valid_elements,gap_cost_f,0.01f);
            gap_cost_f = svmul_n_f64_x(valid_elements,gap_cost_f,avg_qspan001);
            svint64_t gap_cost = svcvt_s64_f64_x(valid_elements,gap_cost_f);
            log_dd = svlsr_n_s64_x(valid_elements,log_dd,1);
            gap_cost = svadd_s64_x(valid_elements,gap_cost,log_dd);

            //const int32_t score = ret->scores[j] + oc - gap_cost;
            svint64_t score = svld1sw_s64(valid_elements,&ret->scores[real_j]);
            score = svadd_s64_x(valid_elements,score,oc);
            score = svsub_s64_x(valid_elements,score,gap_cost);

            // TODO: CAN'T VECTORIZE THIS. THE COMPILER IS ONLY ABLE TO PERFORM
            // ONE REDUCTION.
            // Ideas:
            //     1. j to int32_t and: uint64_t max = (j << 32) & score.  
            //        max = ((int32_t)(max & 0xffffffff) > score) ? max = (j << 32) & score : max;
            // Reduction

            // TODO: reduction on vector register, no scalar register
            //if (score > max_f) {
            //    max_f = score;
            //    max_j = j;
            //}

            int32_t max_local = svmaxv_s64(valid_elements,score);
            if (max_local > max_f) {
                max_f = max_local;
                // WARNING
                svint64_t index = svindex_s64(real_j,1);
                svbool_t max_index = svcmpeq_n_s64(valid_elements,score,max_local);
                max_j = svlastb_s64(max_index,index);
            }
        }
        ret->scores[i] = max_f;
        ret->parents[i] = max_j;
        //if (max_f == 36 && max_j == 18821) {printf("ee\n");exit(0);}
        ret->peak_scores[i] = max_j >= 0 && ret->peak_scores[max_j] > max_f ? ret->peak_scores[max_j] : max_f;
    }
#else // SCALAR VERSION
    printf("Executing SCALAR version");
    for (int64_t i = 0; i < n; ++i) {
        const auto ri = anchors_x[i];
        const int32_t qi = static_cast<int32_t>(anchors_y[i]);
        const int32_t q_spani = qspans[i];

        int64_t max_j = -1;
        int32_t max_f = q_spani;

        while (st < i && ri > anchors_x[st] + max_dist_x) {
            ++st; //predecessor's position is too far
        }
        if (i - st > max_iter) {
            st = i - max_iter; //predecessor's index is too far
        }

        // TODO: Iterate forward to vectorize the loop.
        // for (int64_t j_inv = st; j_inv < i; ++j_inv) {
        //     const int64_t j = (i - 1) - j_inv + st;
        for (int64_t j = i - 1; j >= st; --j) {
            const auto rj = anchors_x[j];
            const int32_t qj = static_cast<int32_t>(anchors_y[j]);

            const int64_t dr = ri - rj;
            const int32_t dq = qi - qj;

            const int32_t dd = std::abs(dr - dq);

            if ((dr == 0 || dq <= 0) ||
               (dq > max_dist_y || dq > max_dist_x) ||
               (dd > bw)) {
                continue;
            }
            // Can not vectorize "continue". Use a mask instead.
            // const bool skip = ((dr == 0 || dq <= 0) ||
            //                    (dq > max_dist_y || dq > max_dist_x) ||
            //                    (dd > bw));

            const int64_t dr_dq_min = (dr < dq) ? dr : dq;
            const int32_t oc = (dr_dq_min < q_spani) ? dr_dq_min : q_spani;

            // TODO: CAN'T VECTORIZE __builtin_clz 
            const int32_t log_dd = (dd) ? ilog2_32(dd) : 0;
            const int32_t gap_cost = static_cast<int>(dd * 0.01f * avg_qspan) + (log_dd >> 1);

            const int32_t score = ret->scores[j] + oc - gap_cost;

            // TODO: CAN'T VECTORIZE THIS. THE COMPILER IS ONLY ABLE TO PERFORM
            // ONE REDUCTION.
            // Ideas:
            //     1. j to int32_t and: uint64_t max = (j << 32) & score.  
            //        max = ((int32_t)(max & 0xffffffff) > score) ? max = (j << 32) & score : max;
            // Reduction
            if (score > max_f) {
                max_f = score;
                max_j = j;
            }
        }
        ret->scores[i] = max_f;
        ret->parents[i] = max_j;
        ret->peak_scores[i] = max_j >= 0 && ret->peak_scores[max_j] > max_f ? ret->peak_scores[max_j] : max_f;
    }
#endif

void host_chain_kernel(std::vector<call_t> &args, std::vector<return_t> &rets, int numThreads) {
    #pragma omp parallel num_threads(numThreads)
    {
        #pragma omp for schedule(dynamic)
        for (size_t batch = 0; batch < args.size(); batch++) {
            call_t *arg = &args[batch];
            return_t *ret = &rets[batch];
            // fprintf(stderr, "%lld\t%f\t%d\t%d\t%d\t%d\n", arg->n, arg->avg_qspan, arg->max_dist_x, arg->max_dist_y, arg->bw, arg->n_segs);
            chain_dp(arg, ret);
        }
    }
}
